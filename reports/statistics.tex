\documentclass{report}
\input{preamble.tex}
\usepackage{pgf}
\usepackage{pgfplots}

\pgfplotsset{
	legend style={rounded corners=2pt},
}

\title{Machine Learning Project Report\\\textbf{Evolutionary Computing\\on Turing machines}\\{\Large Solving the Busy Beaver Problem}}
\author{Jean-Florent Raymond\\\href{mailto:jean-florent.raymond.8795@student.uu.se}{\texttt{jean-florent.raymond.8795@student.uu.se}} \and Emilio Del Tessandoro\\
  \href{mailto:emilio.del_tessandoro.4062@student.uu.se }{\texttt{emilio.del\_tessandoro.4062@student.uu.se }}}
\date{\today}

\begin{document}
%\maketitle

We investigated the efficiency of the evolutionary computing approach over the pure random search adjusting the crossover probability. Setting this probability to 0 means that the algorithm proceed only by mutations, \ie it becomes a random search.

The only cases we tested are:
\begin{itemize}
\item $N = 3$, $M = 2$ (plots \ref{plot32} and \ref{plot32good}),
\item $N = 4$, $M = 2$ (plot \ref{plot42}),
\item $N = 2$, $M = 3$ (plots \ref{plot23}),
\end{itemize}

that are the most difficult ones where the upper bound of the $S$ function is known. The cases where the upper bound is not known are too difficult to solve and hasn't been investigated.

Plot \ref{plot32} show a simple comparison of the two approaches. %In this case the best machine(s) computes 21 steps and the search space size is 16\,777\,216.
Fixed the generations number, the random search obviously explore less compared to the normal evolutionary approach (because no crossovers are performed), so the normal approach should be always more effective. In fact the random search produces on average one third of the machines produced by the plain evolutionary approach (using the same number of generations and a crossover probability of 0.9).
But also considering this fact, the random search is instead more effective.
This happens because it's true that crossovers introduce more ``good'' machines, but is also true that, since the population size is limited, these machines are selected in preference of other ones (machines that instead are kept in the random search).

A more fair comparison is shown in plot \ref{plot32good}. This time the number of steps of the best machine found (average) is plotted against the number of explored machines (average)\footnote{This is not exact. The number of explored machines is strictly less than the shown numbers because we can generate more times the same machine (we didn't add a check for this fact).}. In this case is evident that random search is always better.

Notice that random search is better not only in terms of the ratio ``how good is the found machine'' versus ``number of explored machines'', but also in time complexity. This because producing a new machine via crossover can take time proportional to the transition table size (\ie $O(MN)$), while a mutation is always $O(1)$ in our implementation.

\begin{figure}[t]
\centering
\begin{tikzpicture}
    \begin{semilogxaxis}[
        xlabel=generations,
        ylabel=best found (average),
	xmin=10,
	xmax=1000,
	legend pos=south east]
		
	\addplot[smooth,color=red]   table[x=generations,y=averageSteps] {../results/statistics-non-stationary/3.2.statistics};
	\addlegendentry{normal}
	\addplot[smooth,color=blue] table[x=generations,y=averageSteps] {../results/statistics-non-stationary/3.2.statistics.random};
	\addlegendentry{only random}
    \end{semilogxaxis}
\end{tikzpicture}
\caption[]{Statistics for the case $N=3$, $M=2$. Best machine found (an average over 200 program executions) versus the generations number. This comparison it's not fair since the normal algorithm produces a lot more machines than the (only) random version. But, also considering this fact, the plot shows that the random search is more effective.}
\label{plot32}
\end{figure}


\begin{figure}[b]
\centering
\begin{tikzpicture}
    \begin{semilogxaxis}[
        xlabel=explored machines,
        ylabel=best found (average),
	%width=\textwidth,
	xmin=3000,
	xmax=600000,
	legend pos=south east]
		
	\addplot[smooth,color=red]   table[x=averageExplored,y=averageSteps] {../results/statistics-non-stationary/3.2.statistics};
	\addlegendentry{normal}
	\addplot[smooth,color=blue] table[x=averageExplored,y=averageSteps] {../results/statistics-non-stationary/3.2.statistics.random};
	\addlegendentry{only random}
    \end{semilogxaxis}
\end{tikzpicture}
\caption[]{Statistics for the case $N=3$, $M=2$. Best machine found (an average over 200 program executions) versus the number of explored machines (average). This is a more fair comparison w.r.t the previous plot.}
\label{plot32good}
\end{figure}




\begin{figure}[h]
\centering
\begin{tikzpicture}
    \begin{semilogxaxis}[
        xlabel=explored machines,
        ylabel=best found (average),
	%width=\textwidth,
	xmin=3000,
	xmax=2000000,
	legend pos=south east]
		
	\addplot[smooth,color=red]   table[x=averageExplored,y=averageSteps] {../results/statistics-non-stationary/4.2.statistics};
	\addlegendentry{normal}
	\addplot[smooth,color=blue] table[x=averageExplored,y=averageSteps] {../results/statistics-non-stationary/4.2.statistics.random};
	\addlegendentry{only random}
    \end{semilogxaxis}
\end{tikzpicture}
\caption[]{Statistics for the case $N=4$, $M=2$.}
\label{plot42}
\end{figure}


\begin{figure}[h]
\centering
\begin{tikzpicture}
    \begin{semilogxaxis}[
        xlabel=explored machines,
        ylabel=best found (average),
	%width=\textwidth,
	xmin=3000,
	xmax=2000000,
	legend pos=south east]
		
	\addplot[smooth,color=red]   table[x=averageExplored,y=averageSteps] {../results/statistics-non-stationary/2.3.statistics};
	\addlegendentry{normal}
	\addplot[smooth,color=blue] table[x=averageExplored,y=averageSteps] {../results/statistics-non-stationary/2.3.statistics.random};
	\addlegendentry{only random}
    \end{semilogxaxis}
\end{tikzpicture}
\caption[]{Statistics for the case $N=2$, $M=3$. Exploring $10^6$ machines correspond to use about 3000 generations in the random search: with these settings an optimal machine is found 80\% of the times.}
\label{plot23}
\end{figure}






\section*{With stationary fitness function}
The tests made until now use a non stationary fitness function that, in the first generations, compute only a certain number of steps on the machine, increasing this number of steps in the following generations.

The situation becomes slightly better using a stationary fitness function, approach that is possible since we know the value of the $S$ function for these cases (figures \ref{plot32stationary}, \ref{plot42stationary}, \ref{plot23stationary}). The efficiency of the algorithm, in terms of how many generations are needed for finding a machine that compute $n$ steps, is higher in both cases. Also the difference between the two approaches becomes smaller.

An interesting situation is shown in figure \ref{plot23stationary} where, for some settings, the evolutionary approach is better than the random search.

\begin{figure}[h]
\centering
\begin{tikzpicture}
    \begin{semilogxaxis}[
        xlabel=explored machines,
        ylabel=best found (average),
	%width=\textwidth,
	xmin=3000,
	xmax=600000,
	legend pos=south east]
		
	\addplot[smooth,color=red]   table[x=averageExplored,y=averageSteps] {../results/statistics-stationary/3.2.statistics};
	\addlegendentry{normal}
	\addplot[smooth,color=blue] table[x=averageExplored,y=averageSteps] {../results/statistics-stationary/3.2.statistics.random};
	\addlegendentry{only random}
    \end{semilogxaxis}
\end{tikzpicture}
\caption[]{Statistics for the case $N=3$, $M=2$, stationary fitness function.}
\label{plot32stationary}
\end{figure}


\begin{figure}[h]
\centering
\begin{tikzpicture}
    \begin{semilogxaxis}[
        xlabel=explored machines,
        ylabel=best found (average),
	%width=\textwidth,
	xmin=3000,
	xmax=1000000,
	legend pos=south east]
		
	\addplot[smooth,color=red]   table[x=averageExplored,y=averageSteps] {../results/statistics-stationary/4.2.statistics};
	\addlegendentry{normal}
	\addplot[smooth,color=blue] table[x=averageExplored,y=averageSteps] {../results/statistics-stationary/4.2.statistics.random};
	\addlegendentry{only random}
    \end{semilogxaxis}
\end{tikzpicture}
\caption[]{Statistics for the case $N=4$, $M=2$, stationary fitness function. The idea is the same as figure \ref{plot32stationary} except that here the average is made over 500 program executions.}
\label{plot42stationary}
\end{figure}


\begin{figure}[h]
\centering
\begin{tikzpicture}
    \begin{semilogxaxis}[
        xlabel=explored machines,
        ylabel=best found (average),
	%width=\textwidth,
	xmin=3000,
	xmax=3000000,
	legend pos=south east]
		
	\addplot[smooth,color=red]   table[x=averageExplored,y=averageSteps] {../results/statistics-stationary/2.3.statistics};
	\addlegendentry{normal}
	\addplot[smooth,color=blue] table[x=averageExplored,y=averageSteps] {../results/statistics-stationary/2.3.statistics.random};
	\addlegendentry{only random}
    \end{semilogxaxis}
\end{tikzpicture}
\caption[]{Statistics for the case $N=2$, $M=3$, stationary fitness function. Again what is shown is an average over 500 program executions. This is the only case where the normal approach shows some advantages in respect to the pure random search; but increasing more the number of generations random search becomes better again.}
\label{plot23stationary}
\end{figure}

\end{document}